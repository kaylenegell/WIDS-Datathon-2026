{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26d97e7",
   "metadata": {},
   "source": [
    "# WiDS Datathon 2026 EDA: Kaylee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde3921",
   "metadata": {},
   "source": [
    "## Basic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22ae82fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA & GETTING DATASET SHAPES\n",
      "================================================================================\n",
      "\n",
      "Train shape: (221, 37)\n",
      "\n",
      "Training data first five rows:\n",
      "   event_id  num_perimeters_0_5h  dt_first_last_0_5h  \\\n",
      "0  10892457                    3            4.265188   \n",
      "1  11757157                    2            1.169918   \n",
      "2  11945086                    4            4.777526   \n",
      "3  12044083                    1            0.000000   \n",
      "4  12052347                    2            4.975273   \n",
      "\n",
      "   low_temporal_resolution_0_5h  area_first_ha  area_growth_abs_0_5h  \\\n",
      "0                             0      79.696304              2.875935   \n",
      "1                             0       8.946749              0.000000   \n",
      "2                             0     106.482638              0.000000   \n",
      "3                             1      67.631125              0.000000   \n",
      "4                             0      35.632874              0.000000   \n",
      "\n",
      "   area_growth_rel_0_5h  area_growth_rate_ha_per_h  log1p_area_first  \\\n",
      "0              0.036086                   0.674281          4.390693   \n",
      "1              0.000000                   0.000000          2.297246   \n",
      "2              0.000000                   0.000000          4.677329   \n",
      "3              0.000000                   0.000000          4.228746   \n",
      "4              0.000000                   0.000000          3.600946   \n",
      "\n",
      "   log1p_growth  ...  dist_fit_r2_0_5h  alignment_cos  alignment_abs  \\\n",
      "0      1.354787  ...          0.886373      -0.054649       0.054649   \n",
      "1      0.000000  ...          0.000000      -0.568898       0.568898   \n",
      "2      0.000000  ...          0.000000       0.882385       0.882385   \n",
      "3      0.000000  ...          0.000000       0.000000       0.000000   \n",
      "4      0.000000  ...          0.000000       0.934634       0.934634   \n",
      "\n",
      "   cross_track_component  along_track_speed  event_start_hour  \\\n",
      "0              -1.937219          -0.106026                19   \n",
      "1              -0.000000          -0.000000                 4   \n",
      "2               0.000000           0.000000                22   \n",
      "3               0.000000           0.000000                20   \n",
      "4              -0.000000           0.000000                21   \n",
      "\n",
      "   event_start_dayofweek  event_start_month  time_to_hit_hours  event  \n",
      "0                      4                  5          18.892512      0  \n",
      "1                      4                  6          22.048108      1  \n",
      "2                      4                  8           0.888895      1  \n",
      "3                      5                  8          60.953021      0  \n",
      "4                      5                  7          44.990274      0  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "Test shape: (95, 35)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data and get shapes of datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING DATA & GETTING DATASET SHAPES\")\n",
    "print(\"=\" * 80)\n",
    "train = pd.read_csv('WiDSWorldWide_GlobalDathon26/train.csv')\n",
    "test = pd.read_csv('WiDSWorldWide_GlobalDathon26/test.csv')\n",
    "metadata = pd.read_csv('WiDSWorldWide_GlobalDathon26/metaData.csv')\n",
    "\n",
    "# shape and excerpt of data sets \n",
    "print(f\"\\nTrain shape: {train.shape}\")\n",
    "print(f\"\\nTraining data first five rows:\")\n",
    "print(train.head())\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a802fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA SUMMARY\n",
      "================================================================================\n",
      "Total features: 34\n",
      "Target distribution:\n",
      "  - Event=0 (Censored): 152 (68.8%)\n",
      "  - Event=1 (Hit):      69 (31.2%)\n",
      "\n",
      "Time to hit statistics:\n",
      "  - Overall range: [0.0, 67.0] hours\n",
      "  - For hits (event=1): Mean=10.0h, Median=3.5h\n",
      "\n",
      " No missing values detected!\n"
     ]
    }
   ],
   "source": [
    "# More in depth, basic overview of data\n",
    "target = 'event'\n",
    "time_target = 'time_to_hit_hours'\n",
    "id_col = 'event_id'\n",
    "feature_cols = [col for col in train.columns if col not in [id_col, target, time_target]]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATA SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(f\"  - Event=0 (Censored): {(train[target]==0).sum()} ({(train[target]==0).mean()*100:.1f}%)\")\n",
    "print(f\"  - Event=1 (Hit):      {(train[target]==1).sum()} ({(train[target]==1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTime to hit statistics:\")\n",
    "print(f\"  - Overall range: [{train[time_target].min():.1f}, {train[time_target].max():.1f}] hours\")\n",
    "hits_only = train[train[target] == 1]\n",
    "print(f\"  - For hits (event=1): Mean={hits_only[time_target].mean():.1f}h, Median={hits_only[time_target].median():.1f}h\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = train[feature_cols].isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(f\"\\n No missing values detected!\")\n",
    "else:\n",
    "    print(f\"\\n Missing values found in {(missing > 0).sum()} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6d172",
   "metadata": {},
   "source": [
    "There are ~69% of the data points in the training set that are a censored event, and ~31% that the fire hits. The range provided for the data is 72 hours, but no data is measured that hits past 67 hours. The average time in the event of fire hitting is 10 hours after first obsservation, while the median is much quicker at 3.5 hours. No missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3207ae7",
   "metadata": {},
   "source": [
    "## Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b7728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURES BY CATEGORY\n",
      "================================================================================\n",
      "\n",
      "CENTROID_KINEMATICS (5 features):\n",
      "  - centroid_displacement_m\n",
      "  - centroid_speed_m_per_h\n",
      "  - spread_bearing_deg\n",
      "  - spread_bearing_sin\n",
      "  - spread_bearing_cos\n",
      "\n",
      "DIRECTIONALITY (4 features):\n",
      "  - alignment_cos\n",
      "  - alignment_abs\n",
      "  - cross_track_component\n",
      "  - along_track_speed\n",
      "\n",
      "DISTANCE (9 features):\n",
      "  - dist_min_ci_0_5h\n",
      "  - dist_std_ci_0_5h\n",
      "  - dist_change_ci_0_5h\n",
      "  - dist_slope_ci_0_5h\n",
      "  - closing_speed_m_per_h\n",
      "  ... and 4 more\n",
      "\n",
      "GROWTH (10 features):\n",
      "  - area_first_ha\n",
      "  - area_growth_abs_0_5h\n",
      "  - area_growth_rel_0_5h\n",
      "  - area_growth_rate_ha_per_h\n",
      "  - log1p_area_first\n",
      "  ... and 5 more\n",
      "\n",
      "TEMPORAL_COVERAGE (3 features):\n",
      "  - num_perimeters_0_5h\n",
      "  - dt_first_last_0_5h\n",
      "  - low_temporal_resolution_0_5h\n",
      "\n",
      "TEMPORAL_METADATA (3 features):\n",
      "  - event_start_hour\n",
      "  - event_start_dayofweek\n",
      "  - event_start_month\n"
     ]
    }
   ],
   "source": [
    "# using metadata to map features by type \n",
    "category_map = dict(zip(metadata['column'], metadata['category']))\n",
    "feature_categories = {}\n",
    "for col in feature_cols:\n",
    "    cat = category_map.get(col, 'unknown')\n",
    "    if cat not in feature_categories:\n",
    "        feature_categories[cat] = []\n",
    "    feature_categories[cat].append(col)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FEATURES BY CATEGORY\")\n",
    "print(f\"{'='*80}\")\n",
    "for cat, cols in sorted(feature_categories.items()):\n",
    "    print(f\"\\n{cat.upper()} ({len(cols)} features):\")\n",
    "    for col in cols[:5]:  # Show first 5\n",
    "        print(f\"  - {col}\")\n",
    "    if len(cols) > 5:\n",
    "        print(f\"  ... and {len(cols)-5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520b466",
   "metadata": {},
   "source": [
    "## Statistical Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908f148b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HIGH SPARSITY FEATURES (>80% zeros)\n",
      "================================================================================\n",
      "                                mean         std  zeros_pct\n",
      "projected_advance_m        10.286955  128.652678  91.855204\n",
      "closing_speed_abs_m_per_h   3.661135   26.690409  91.855204\n",
      "closing_speed_m_per_h       2.021403   26.865184  91.855204\n",
      "dist_change_ci_0_5h       -10.286955  128.652678  91.855204\n",
      "dist_fit_r2_0_5h            0.046000    0.171690  91.402715\n",
      "dist_std_ci_0_5h            8.079022   63.184352  91.402715\n",
      "log1p_growth                0.389346    1.340348  89.140271\n",
      "area_growth_abs_0_5h       26.332398  187.437018  88.687783\n",
      "spread_bearing_sin          0.053662    0.285193  88.687783\n",
      "cross_track_component       1.617188   37.789199  88.687783\n"
     ]
    }
   ],
   "source": [
    "# create statistical summary of features, any that have a large amount of zeroes \n",
    "stats_summary = train[feature_cols].describe().T\n",
    "stats_summary['zeros_pct'] = (train[feature_cols] == 0).sum() / len(train) * 100\n",
    "stats_summary['skewness'] = train[feature_cols].skew()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"HIGH SPARSITY FEATURES (>80% zeros)\")\n",
    "print(f\"{'='*80}\")\n",
    "high_zeros = stats_summary[stats_summary['zeros_pct'] > 80].sort_values('zeros_pct', ascending=False)\n",
    "if len(high_zeros) > 0:\n",
    "    print(high_zeros[['mean', 'std', 'zeros_pct']].head(10))\n",
    "else:\n",
    "    print(\"None found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a857c",
   "metadata": {},
   "source": [
    "10 of the features in dataset have high sparsity, including a large amount of zero values. Further exploration into indications needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a50d0",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39a0a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 10 FEATURES CORRELATED WITH TARGET\n",
      "================================================================================\n",
      "dist_min_ci_0_5h                        : -0.4814\n",
      "low_temporal_resolution_0_5h            : -0.3791\n",
      "num_perimeters_0_5h                     : +0.3705\n",
      "dt_first_last_0_5h                      : +0.3530\n",
      "alignment_abs                           : +0.3491\n",
      "spread_bearing_cos                      : -0.3232\n",
      "log1p_growth                            : +0.2927\n",
      "spread_bearing_deg                      : +0.2810\n",
      "log_area_ratio_0_5h                     : +0.2293\n",
      "radial_growth_rate_m_per_h              : +0.2150\n",
      "\n",
      "Searching for highly correlated feature pairs (|r| > 0.8)...\n",
      "Found 65 highly correlated pairs\n",
      "\n",
      "Top 5 correlated pairs:\n",
      "  area_growth_rel_0_5h           <-> relative_growth_0_5h          : 1.000\n",
      "  dist_change_ci_0_5h            <-> projected_advance_m           : -1.000\n",
      "  dist_change_ci_0_5h            <-> closing_speed_m_per_h         : -0.998\n",
      "  closing_speed_m_per_h          <-> projected_advance_m           : 0.998\n",
      "  dist_std_ci_0_5h               <-> closing_speed_abs_m_per_h     : 0.997\n"
     ]
    }
   ],
   "source": [
    "# Correlation with target\n",
    "correlations = train[feature_cols].corrwith(train[target])\n",
    "correlations_sorted = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 10 FEATURES CORRELATED WITH TARGET\")\n",
    "print(f\"{'='*80}\")\n",
    "for feat in correlations_sorted.head(10).index:\n",
    "    corr_val = correlations[feat]\n",
    "    print(f\"{feat:40s}: {corr_val:+.4f}\")\n",
    "\n",
    "# Feature intercorrelations\n",
    "print(f\"\\nSearching for highly correlated feature pairs (|r| > 0.8)...\")\n",
    "corr_matrix = train[feature_cols].corr()\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': corr_matrix.columns[i],\n",
    "                'feature_2': corr_matrix.columns[j],\n",
    "                'correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(high_corr_pairs)} highly correlated pairs\")\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(\"\\nTop 5 correlated pairs:\")\n",
    "    for pair in sorted(high_corr_pairs, key=lambda x: abs(x['correlation']), reverse=True)[:5]:\n",
    "        print(f\"  {pair['feature_1']:30s} <-> {pair['feature_2']:30s}: {pair['correlation']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
